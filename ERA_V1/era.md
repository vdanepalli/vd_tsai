


## S1 Fundamentals of AI

- Vision AI relied on Convolutions and NLP on RNN/LSTM architectures
- Since 2017, transformers changed everything -- Attention is all you need!; 
  - increase in parameters and data, accuracy increases -- linear; earlier, it wasn't like this
  - Encoder and Decoder
  - We do not need supervised data, model predicts the missing word -- fill in the blanks -- do not need to annotate data
    - hallucinations
- Multimodal models -- text audio video etc
- Loss functions -- model needs to learn the context;



<br/><br/>

- Papers
  - Attention is all you need
  - YOLO - You Only Look Once


## S2 Exploring Neural Network Architectures


## S3 Git and Python Essentials


## S4 Building the First Neural Networks



## S5 Introduction to PyTorch


## S6 Backpropagation and Advanced Architectures


## S7 In-Depth Coding Practice


## S8 Advanced Techniques and Optimizations


## S9 Data Augmentation and Visualization


## S10 PyTorch Lightning and AI Application Development


## S11 Residual Connections in CNNs and FC Layers


## S12 Building and Deploying AI Applications


## S13 YOLO and Object Detection Techniques


## S14 Multi-GPU Training and Scalable Model Serving


## S15 UNETs, Variational AutoEncoders, and Applications


## S16 Transformers and Advanced Embedding Techniques


## S17 Encoder Architectures and BERT


## S18 Masked AutoEncoders and Vision Transformers


## S19 Decoders and Generative Pre-trained Transformers



## S21 Training and Fine-tuning Large Language Models


## S22 CLIP Models and Training


## S23 Generative Art and Stable Diffusion



## S24 Automatic Speech Recognition Fundamentals


## S25 Reinforcement Learning Part I


## S26 Reinforcement Learning Part II


## S27 Reinforcement Learning from Human Feedback


## S28 Training ChatGPT from Scratch


## S29 Training Multimodal


## S30 Capstone Project